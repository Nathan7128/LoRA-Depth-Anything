{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfd38ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\Git\\LoRA-Depth-Anything\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoImageProcessor, \n",
    "    AutoModelForDepthEstimation, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a692bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, pairs_list, images_path, depth_npy_path, image_processor):\n",
    "        self.pairs_list = pairs_list  # Liste des paires (image_filename, depth_filename)\n",
    "        self.images_path = images_path\n",
    "        self.depth_npy_path = depth_npy_path\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, depth_npy_name = self.pairs_list[idx]\n",
    "        \n",
    "        # Chargement à la demande\n",
    "        image_file = os.path.join(self.images_path, image_name)\n",
    "        depth_npy_file = os.path.join(self.depth_npy_path, depth_npy_name)\n",
    "        \n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        depth_numpy = np.load(depth_npy_file)\n",
    "        \n",
    "        # CORRECTION ICI : Gestion des canaux de profondeur\n",
    "        # Si la depth a 3 canaux (H, W, 3), on ne garde que le premier (H, W)\n",
    "        if len(depth_numpy.shape) == 3:\n",
    "            depth_numpy = depth_numpy[:, :, 0]\n",
    "        \n",
    "        # Prétraitement de l'image\n",
    "        inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Conversion en Tensor\n",
    "        depth_tensor = torch.from_numpy(depth_numpy).float()\n",
    "        \n",
    "        # On a maintenant une forme (H, W). \n",
    "        # On ajoute (Batch, Channel) pour obtenir (1, 1, H, W) requis par interpolate\n",
    "        depth_tensor = depth_tensor.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Récupération de la taille cible\n",
    "        target_size = inputs['pixel_values'].shape[-2:]\n",
    "        \n",
    "        # Interpolation\n",
    "        depth_resized = F.interpolate(depth_tensor, size=target_size, mode='nearest')\n",
    "        \n",
    "        # On retire les dimensions pour revenir à (H, W) pour les labels\n",
    "        depth_resized = depth_resized.squeeze()\n",
    "\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'labels': depth_resized\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557b9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# 2. Chargement du Modèle et Processor\n",
    "model_id = \"depth-anything/Depth-Anything-V2-Small-hf\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForDepthEstimation.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b162b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Préparation des données (comme dans ton code original)\n",
    "# Assure-toi que 'dataset' (ta classe DatasetImages) est bien instancié avant\n",
    "dataset_path = \"DATASET_DEVOIR\"\n",
    "images_path = os.path.join(dataset_path, \"images\")\n",
    "depth_npy_path = os.path.join(dataset_path, \"depth\")\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "depth_files = sorted(os.listdir(depth_npy_path))\n",
    "# Filtrer pour s'assurer que les fichiers correspondent bien si nécessaire\n",
    "all_pairs = list(zip(image_files, depth_files))\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "split_idx = int(0.8 * len(all_pairs))\n",
    "train_pairs = all_pairs[:split_idx]\n",
    "eval_pairs = all_pairs[split_idx:]\n",
    "\n",
    "train_dataset = DepthDataset(train_pairs, images_path, depth_npy_path, image_processor)\n",
    "eval_dataset = DepthDataset(eval_pairs, images_path, depth_npy_path, image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625ede33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noms de toutes les couches du modèle :\n",
      "backbone\n",
      "backbone.embeddings\n",
      "backbone.embeddings.patch_embeddings\n",
      "backbone.embeddings.patch_embeddings.projection\n",
      "backbone.embeddings.dropout\n",
      "backbone.encoder\n",
      "backbone.encoder.layer\n",
      "backbone.encoder.layer.0\n",
      "backbone.encoder.layer.0.norm1\n",
      "backbone.encoder.layer.0.attention\n",
      "backbone.encoder.layer.0.attention.attention\n",
      "backbone.encoder.layer.0.attention.attention.query\n",
      "backbone.encoder.layer.0.attention.attention.key\n",
      "backbone.encoder.layer.0.attention.attention.value\n",
      "backbone.encoder.layer.0.attention.output\n",
      "backbone.encoder.layer.0.attention.output.dense\n",
      "backbone.encoder.layer.0.attention.output.dropout\n",
      "backbone.encoder.layer.0.layer_scale1\n",
      "backbone.encoder.layer.0.drop_path\n",
      "backbone.encoder.layer.0.norm2\n",
      "backbone.encoder.layer.0.mlp\n",
      "backbone.encoder.layer.0.mlp.fc1\n",
      "backbone.encoder.layer.0.mlp.activation\n",
      "backbone.encoder.layer.0.mlp.fc2\n",
      "backbone.encoder.layer.0.layer_scale2\n",
      "backbone.encoder.layer.1\n",
      "backbone.encoder.layer.1.norm1\n",
      "backbone.encoder.layer.1.attention\n",
      "backbone.encoder.layer.1.attention.attention\n",
      "backbone.encoder.layer.1.attention.attention.query\n",
      "backbone.encoder.layer.1.attention.attention.key\n",
      "backbone.encoder.layer.1.attention.attention.value\n",
      "backbone.encoder.layer.1.attention.output\n",
      "backbone.encoder.layer.1.attention.output.dense\n",
      "backbone.encoder.layer.1.attention.output.dropout\n",
      "backbone.encoder.layer.1.layer_scale1\n",
      "backbone.encoder.layer.1.drop_path\n",
      "backbone.encoder.layer.1.norm2\n",
      "backbone.encoder.layer.1.mlp\n",
      "backbone.encoder.layer.1.mlp.fc1\n",
      "backbone.encoder.layer.1.mlp.activation\n",
      "backbone.encoder.layer.1.mlp.fc2\n",
      "backbone.encoder.layer.1.layer_scale2\n",
      "backbone.encoder.layer.2\n",
      "backbone.encoder.layer.2.norm1\n",
      "backbone.encoder.layer.2.attention\n",
      "backbone.encoder.layer.2.attention.attention\n",
      "backbone.encoder.layer.2.attention.attention.query\n",
      "backbone.encoder.layer.2.attention.attention.key\n",
      "backbone.encoder.layer.2.attention.attention.value\n",
      "backbone.encoder.layer.2.attention.output\n",
      "backbone.encoder.layer.2.attention.output.dense\n",
      "backbone.encoder.layer.2.attention.output.dropout\n",
      "backbone.encoder.layer.2.layer_scale1\n",
      "backbone.encoder.layer.2.drop_path\n",
      "backbone.encoder.layer.2.norm2\n",
      "backbone.encoder.layer.2.mlp\n",
      "backbone.encoder.layer.2.mlp.fc1\n",
      "backbone.encoder.layer.2.mlp.activation\n",
      "backbone.encoder.layer.2.mlp.fc2\n",
      "backbone.encoder.layer.2.layer_scale2\n",
      "backbone.encoder.layer.3\n",
      "backbone.encoder.layer.3.norm1\n",
      "backbone.encoder.layer.3.attention\n",
      "backbone.encoder.layer.3.attention.attention\n",
      "backbone.encoder.layer.3.attention.attention.query\n",
      "backbone.encoder.layer.3.attention.attention.key\n",
      "backbone.encoder.layer.3.attention.attention.value\n",
      "backbone.encoder.layer.3.attention.output\n",
      "backbone.encoder.layer.3.attention.output.dense\n",
      "backbone.encoder.layer.3.attention.output.dropout\n",
      "backbone.encoder.layer.3.layer_scale1\n",
      "backbone.encoder.layer.3.drop_path\n",
      "backbone.encoder.layer.3.norm2\n",
      "backbone.encoder.layer.3.mlp\n",
      "backbone.encoder.layer.3.mlp.fc1\n",
      "backbone.encoder.layer.3.mlp.activation\n",
      "backbone.encoder.layer.3.mlp.fc2\n",
      "backbone.encoder.layer.3.layer_scale2\n",
      "backbone.encoder.layer.4\n",
      "backbone.encoder.layer.4.norm1\n",
      "backbone.encoder.layer.4.attention\n",
      "backbone.encoder.layer.4.attention.attention\n",
      "backbone.encoder.layer.4.attention.attention.query\n",
      "backbone.encoder.layer.4.attention.attention.key\n",
      "backbone.encoder.layer.4.attention.attention.value\n",
      "backbone.encoder.layer.4.attention.output\n",
      "backbone.encoder.layer.4.attention.output.dense\n",
      "backbone.encoder.layer.4.attention.output.dropout\n",
      "backbone.encoder.layer.4.layer_scale1\n",
      "backbone.encoder.layer.4.drop_path\n",
      "backbone.encoder.layer.4.norm2\n",
      "backbone.encoder.layer.4.mlp\n",
      "backbone.encoder.layer.4.mlp.fc1\n",
      "backbone.encoder.layer.4.mlp.activation\n",
      "backbone.encoder.layer.4.mlp.fc2\n",
      "backbone.encoder.layer.4.layer_scale2\n",
      "backbone.encoder.layer.5\n",
      "backbone.encoder.layer.5.norm1\n",
      "backbone.encoder.layer.5.attention\n",
      "backbone.encoder.layer.5.attention.attention\n",
      "backbone.encoder.layer.5.attention.attention.query\n",
      "backbone.encoder.layer.5.attention.attention.key\n",
      "backbone.encoder.layer.5.attention.attention.value\n",
      "backbone.encoder.layer.5.attention.output\n",
      "backbone.encoder.layer.5.attention.output.dense\n",
      "backbone.encoder.layer.5.attention.output.dropout\n",
      "backbone.encoder.layer.5.layer_scale1\n",
      "backbone.encoder.layer.5.drop_path\n",
      "backbone.encoder.layer.5.norm2\n",
      "backbone.encoder.layer.5.mlp\n",
      "backbone.encoder.layer.5.mlp.fc1\n",
      "backbone.encoder.layer.5.mlp.activation\n",
      "backbone.encoder.layer.5.mlp.fc2\n",
      "backbone.encoder.layer.5.layer_scale2\n",
      "backbone.encoder.layer.6\n",
      "backbone.encoder.layer.6.norm1\n",
      "backbone.encoder.layer.6.attention\n",
      "backbone.encoder.layer.6.attention.attention\n",
      "backbone.encoder.layer.6.attention.attention.query\n",
      "backbone.encoder.layer.6.attention.attention.key\n",
      "backbone.encoder.layer.6.attention.attention.value\n",
      "backbone.encoder.layer.6.attention.output\n",
      "backbone.encoder.layer.6.attention.output.dense\n",
      "backbone.encoder.layer.6.attention.output.dropout\n",
      "backbone.encoder.layer.6.layer_scale1\n",
      "backbone.encoder.layer.6.drop_path\n",
      "backbone.encoder.layer.6.norm2\n",
      "backbone.encoder.layer.6.mlp\n",
      "backbone.encoder.layer.6.mlp.fc1\n",
      "backbone.encoder.layer.6.mlp.activation\n",
      "backbone.encoder.layer.6.mlp.fc2\n",
      "backbone.encoder.layer.6.layer_scale2\n",
      "backbone.encoder.layer.7\n",
      "backbone.encoder.layer.7.norm1\n",
      "backbone.encoder.layer.7.attention\n",
      "backbone.encoder.layer.7.attention.attention\n",
      "backbone.encoder.layer.7.attention.attention.query\n",
      "backbone.encoder.layer.7.attention.attention.key\n",
      "backbone.encoder.layer.7.attention.attention.value\n",
      "backbone.encoder.layer.7.attention.output\n",
      "backbone.encoder.layer.7.attention.output.dense\n",
      "backbone.encoder.layer.7.attention.output.dropout\n",
      "backbone.encoder.layer.7.layer_scale1\n",
      "backbone.encoder.layer.7.drop_path\n",
      "backbone.encoder.layer.7.norm2\n",
      "backbone.encoder.layer.7.mlp\n",
      "backbone.encoder.layer.7.mlp.fc1\n",
      "backbone.encoder.layer.7.mlp.activation\n",
      "backbone.encoder.layer.7.mlp.fc2\n",
      "backbone.encoder.layer.7.layer_scale2\n",
      "backbone.encoder.layer.8\n",
      "backbone.encoder.layer.8.norm1\n",
      "backbone.encoder.layer.8.attention\n",
      "backbone.encoder.layer.8.attention.attention\n",
      "backbone.encoder.layer.8.attention.attention.query\n",
      "backbone.encoder.layer.8.attention.attention.key\n",
      "backbone.encoder.layer.8.attention.attention.value\n",
      "backbone.encoder.layer.8.attention.output\n",
      "backbone.encoder.layer.8.attention.output.dense\n",
      "backbone.encoder.layer.8.attention.output.dropout\n",
      "backbone.encoder.layer.8.layer_scale1\n",
      "backbone.encoder.layer.8.drop_path\n",
      "backbone.encoder.layer.8.norm2\n",
      "backbone.encoder.layer.8.mlp\n",
      "backbone.encoder.layer.8.mlp.fc1\n",
      "backbone.encoder.layer.8.mlp.activation\n",
      "backbone.encoder.layer.8.mlp.fc2\n",
      "backbone.encoder.layer.8.layer_scale2\n",
      "backbone.encoder.layer.9\n",
      "backbone.encoder.layer.9.norm1\n",
      "backbone.encoder.layer.9.attention\n",
      "backbone.encoder.layer.9.attention.attention\n",
      "backbone.encoder.layer.9.attention.attention.query\n",
      "backbone.encoder.layer.9.attention.attention.key\n",
      "backbone.encoder.layer.9.attention.attention.value\n",
      "backbone.encoder.layer.9.attention.output\n",
      "backbone.encoder.layer.9.attention.output.dense\n",
      "backbone.encoder.layer.9.attention.output.dropout\n",
      "backbone.encoder.layer.9.layer_scale1\n",
      "backbone.encoder.layer.9.drop_path\n",
      "backbone.encoder.layer.9.norm2\n",
      "backbone.encoder.layer.9.mlp\n",
      "backbone.encoder.layer.9.mlp.fc1\n",
      "backbone.encoder.layer.9.mlp.activation\n",
      "backbone.encoder.layer.9.mlp.fc2\n",
      "backbone.encoder.layer.9.layer_scale2\n",
      "backbone.encoder.layer.10\n",
      "backbone.encoder.layer.10.norm1\n",
      "backbone.encoder.layer.10.attention\n",
      "backbone.encoder.layer.10.attention.attention\n",
      "backbone.encoder.layer.10.attention.attention.query\n",
      "backbone.encoder.layer.10.attention.attention.key\n",
      "backbone.encoder.layer.10.attention.attention.value\n",
      "backbone.encoder.layer.10.attention.output\n",
      "backbone.encoder.layer.10.attention.output.dense\n",
      "backbone.encoder.layer.10.attention.output.dropout\n",
      "backbone.encoder.layer.10.layer_scale1\n",
      "backbone.encoder.layer.10.drop_path\n",
      "backbone.encoder.layer.10.norm2\n",
      "backbone.encoder.layer.10.mlp\n",
      "backbone.encoder.layer.10.mlp.fc1\n",
      "backbone.encoder.layer.10.mlp.activation\n",
      "backbone.encoder.layer.10.mlp.fc2\n",
      "backbone.encoder.layer.10.layer_scale2\n",
      "backbone.encoder.layer.11\n",
      "backbone.encoder.layer.11.norm1\n",
      "backbone.encoder.layer.11.attention\n",
      "backbone.encoder.layer.11.attention.attention\n",
      "backbone.encoder.layer.11.attention.attention.query\n",
      "backbone.encoder.layer.11.attention.attention.key\n",
      "backbone.encoder.layer.11.attention.attention.value\n",
      "backbone.encoder.layer.11.attention.output\n",
      "backbone.encoder.layer.11.attention.output.dense\n",
      "backbone.encoder.layer.11.attention.output.dropout\n",
      "backbone.encoder.layer.11.layer_scale1\n",
      "backbone.encoder.layer.11.drop_path\n",
      "backbone.encoder.layer.11.norm2\n",
      "backbone.encoder.layer.11.mlp\n",
      "backbone.encoder.layer.11.mlp.fc1\n",
      "backbone.encoder.layer.11.mlp.activation\n",
      "backbone.encoder.layer.11.mlp.fc2\n",
      "backbone.encoder.layer.11.layer_scale2\n",
      "backbone.layernorm\n",
      "neck\n",
      "neck.reassemble_stage\n",
      "neck.reassemble_stage.layers\n",
      "neck.reassemble_stage.layers.0\n",
      "neck.reassemble_stage.layers.0.projection\n",
      "neck.reassemble_stage.layers.0.resize\n",
      "neck.reassemble_stage.layers.1\n",
      "neck.reassemble_stage.layers.1.projection\n",
      "neck.reassemble_stage.layers.1.resize\n",
      "neck.reassemble_stage.layers.2\n",
      "neck.reassemble_stage.layers.2.projection\n",
      "neck.reassemble_stage.layers.2.resize\n",
      "neck.reassemble_stage.layers.3\n",
      "neck.reassemble_stage.layers.3.projection\n",
      "neck.reassemble_stage.layers.3.resize\n",
      "neck.convs\n",
      "neck.convs.0\n",
      "neck.convs.1\n",
      "neck.convs.2\n",
      "neck.convs.3\n",
      "neck.fusion_stage\n",
      "neck.fusion_stage.layers\n",
      "neck.fusion_stage.layers.0\n",
      "neck.fusion_stage.layers.0.projection\n",
      "neck.fusion_stage.layers.0.residual_layer1\n",
      "neck.fusion_stage.layers.0.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.0.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.0.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.0.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.0.residual_layer2\n",
      "neck.fusion_stage.layers.0.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.0.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.0.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.0.residual_layer2.convolution2\n",
      "neck.fusion_stage.layers.1\n",
      "neck.fusion_stage.layers.1.projection\n",
      "neck.fusion_stage.layers.1.residual_layer1\n",
      "neck.fusion_stage.layers.1.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.1.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.1.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.1.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.1.residual_layer2\n",
      "neck.fusion_stage.layers.1.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.1.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.1.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.1.residual_layer2.convolution2\n",
      "neck.fusion_stage.layers.2\n",
      "neck.fusion_stage.layers.2.projection\n",
      "neck.fusion_stage.layers.2.residual_layer1\n",
      "neck.fusion_stage.layers.2.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.2.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.2.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.2.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.2.residual_layer2\n",
      "neck.fusion_stage.layers.2.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.2.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.2.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.2.residual_layer2.convolution2\n",
      "neck.fusion_stage.layers.3\n",
      "neck.fusion_stage.layers.3.projection\n",
      "neck.fusion_stage.layers.3.residual_layer1\n",
      "neck.fusion_stage.layers.3.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.3.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.3.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.3.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.3.residual_layer2\n",
      "neck.fusion_stage.layers.3.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.3.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.3.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.3.residual_layer2.convolution2\n",
      "head\n",
      "head.conv1\n",
      "head.conv2\n",
      "head.activation1\n",
      "head.conv3\n",
      "head.activation2\n"
     ]
    }
   ],
   "source": [
    "# Afficher les noms de toutes les couches (modules) du modèle\n",
    "print(\"Noms de toutes les couches du modèle :\")\n",
    "for name, module in model.named_modules():\n",
    "    if name:  # Éviter la racine vide\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073cf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration LoRA Correcte pour la Vision\n",
    "# On cible tous les modules linéaires du Transformer pour un meilleur apprentissage\n",
    "# On retire 'task_type' pour éviter l'erreur \"input_ids\"\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\", \"fc1\", \"fc2\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77586d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,327,104 || all params: 26,112,193 || trainable%: 5.0823\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17db4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Trainer Personnalisé pour gérer la Loss et les NaNs\n",
    "class DepthTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "        \n",
    "        # L'output du modèle peut être légèrement différent de la taille d'entrée (padding)\n",
    "        # On s'assure que la prédiction matche les labels\n",
    "        if predicted_depth.shape[-2:] != labels.shape[-2:]:\n",
    "            predicted_depth = F.interpolate(\n",
    "                predicted_depth.unsqueeze(1), \n",
    "                size=labels.shape[-2:], \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            ).squeeze(1)\n",
    "\n",
    "        # Masquage des valeurs invalides (NaNs ou inf)\n",
    "        # On suppose que la profondeur valide est > 0 et n'est pas NaN\n",
    "        valid_mask = ~torch.isnan(labels) & ~torch.isinf(labels) & (labels > 0)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predicted_depth.device, requires_grad=True)\n",
    "\n",
    "        # Calcul de la Loss (L1 Loss est souvent mieux pour la profondeur que MSE)\n",
    "        loss = F.l1_loss(predicted_depth[valid_mask], labels[valid_mask])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d10e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Arguments d'entraînement\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output_depth_lora\",\n",
    "    remove_unused_columns=False, # Important pour garder 'labels'\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4, # Un peu plus bas pour LoRA\n",
    "    per_device_train_batch_size=4, # Ajuste selon ta VRAM (128 est énorme pour des images)\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4, # Simule un batch plus grand\n",
    "    fp16=True,\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"], # Indique au Trainer de ne pas supprimer cette colonne\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc639693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de collation simple\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8499ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\Git\\LoRA-Depth-Anything\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1357: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/150 00:29 < 09:47, 0.24 it/s, Epoch 2.67/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>329.557007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>325.189728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Lancement\n",
    "trainer = DepthTrainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6000f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872ebe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e2e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c1c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
