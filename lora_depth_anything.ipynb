{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd38ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\Git\\LoRA-Depth-Anything\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoImageProcessor, \n",
    "    AutoModelForDepthEstimation, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a692bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, pairs_list, images_path, depth_npy_path, image_processor):\n",
    "        self.pairs_list = pairs_list  # Liste des paires (image_filename, depth_filename)\n",
    "        self.images_path = images_path\n",
    "        self.depth_npy_path = depth_npy_path\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, depth_npy_name = self.pairs_list[idx]\n",
    "        \n",
    "        # Chargement à la demande\n",
    "        image_file = os.path.join(self.images_path, image_name)\n",
    "        depth_npy_file = os.path.join(self.depth_npy_path, depth_npy_name)\n",
    "        \n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        depth_npy = np.load(depth_npy_file)\n",
    "        \n",
    "        # CORRECTION ICI : Gestion des canaux de profondeur\n",
    "        # Si la depth a 3 canaux (H, W, 3), on ne garde que le premier (H, W)\n",
    "        if len(depth_npy.shape) == 3:\n",
    "            depth_npy = depth_npy[:, :, 0]\n",
    "        \n",
    "        # Prétraitement de l'image\n",
    "        inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Conversion en Tensor\n",
    "        depth_tensor = torch.from_numpy(depth_npy).float()\n",
    "        \n",
    "        # On a maintenant une forme (H, W). \n",
    "        # On ajoute (Batch, Channel) pour obtenir (1, 1, H, W) requis par interpolate\n",
    "        depth_tensor = depth_tensor.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Récupération de la taille cible\n",
    "        target_size = inputs['pixel_values'].shape[-2:]\n",
    "        \n",
    "        # Interpolation\n",
    "        depth_resized = F.interpolate(depth_tensor, size=target_size, mode='nearest')\n",
    "        \n",
    "        # On retire les dimensions pour revenir à (H, W) pour les labels\n",
    "        depth_resized = depth_resized.squeeze()\n",
    "\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'labels': depth_resized,\n",
    "            'image': image  # Ajouter l'image originale pour visualisation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557b9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# 2. Chargement du Modèle et Processor\n",
    "model_id = \"depth-anything/Depth-Anything-V2-Small-hf\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForDepthEstimation.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Préparation des données (comme dans ton code original)\n",
    "# Assure-toi que 'dataset' (ta classe DatasetImages) est bien instancié avant\n",
    "dataset_path = \"DATASET_DEVOIR\"\n",
    "images_path = os.path.join(dataset_path, \"images\")\n",
    "depth_npy_path = os.path.join(dataset_path, \"depth\")\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "depth_files = sorted(os.listdir(depth_npy_path))\n",
    "# Filtrer pour s'assurer que les fichiers correspondent bien si nécessaire\n",
    "all_pairs = list(zip(image_files, depth_files))\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "total = len(all_pairs)\n",
    "train_split = int(0.7 * total)\n",
    "eval_split = int(0.85 * total)\n",
    "\n",
    "train_pairs = all_pairs[:train_split]\n",
    "eval_pairs = all_pairs[train_split:eval_split]\n",
    "test_pairs = all_pairs[eval_split:]\n",
    "\n",
    "train_dataset = DepthDataset(train_pairs, images_path, depth_npy_path, image_processor)\n",
    "eval_dataset = DepthDataset(eval_pairs, images_path, depth_npy_path, image_processor)\n",
    "test_dataset = DepthDataset(test_pairs, images_path, depth_npy_path, image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625ede33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noms de toutes les couches du modèle :\n",
      "backbone\n",
      "backbone.embeddings\n",
      "backbone.embeddings.patch_embeddings\n",
      "backbone.embeddings.patch_embeddings.projection\n",
      "backbone.embeddings.dropout\n",
      "backbone.encoder\n",
      "backbone.encoder.layer\n",
      "backbone.encoder.layer.0\n",
      "backbone.encoder.layer.0.norm1\n",
      "backbone.encoder.layer.0.attention\n",
      "backbone.encoder.layer.0.attention.attention\n",
      "backbone.encoder.layer.0.attention.attention.query\n",
      "backbone.encoder.layer.0.attention.attention.key\n",
      "backbone.encoder.layer.0.attention.attention.value\n",
      "backbone.encoder.layer.0.attention.output\n",
      "backbone.encoder.layer.0.attention.output.dense\n",
      "backbone.encoder.layer.0.attention.output.dropout\n",
      "backbone.encoder.layer.0.layer_scale1\n",
      "backbone.encoder.layer.0.drop_path\n",
      "backbone.encoder.layer.0.norm2\n",
      "backbone.encoder.layer.0.mlp\n",
      "backbone.encoder.layer.0.mlp.fc1\n",
      "backbone.encoder.layer.0.mlp.activation\n",
      "backbone.encoder.layer.0.mlp.fc2\n",
      "backbone.encoder.layer.0.layer_scale2\n",
      "backbone.encoder.layer.1\n",
      "backbone.encoder.layer.1.norm1\n",
      "backbone.encoder.layer.1.attention\n",
      "backbone.encoder.layer.1.attention.attention\n",
      "backbone.encoder.layer.1.attention.attention.query\n",
      "backbone.encoder.layer.1.attention.attention.key\n",
      "backbone.encoder.layer.1.attention.attention.value\n",
      "backbone.encoder.layer.1.attention.output\n",
      "backbone.encoder.layer.1.attention.output.dense\n",
      "backbone.encoder.layer.1.attention.output.dropout\n",
      "backbone.encoder.layer.1.layer_scale1\n",
      "backbone.encoder.layer.1.drop_path\n",
      "backbone.encoder.layer.1.norm2\n",
      "backbone.encoder.layer.1.mlp\n",
      "backbone.encoder.layer.1.mlp.fc1\n",
      "backbone.encoder.layer.1.mlp.activation\n",
      "backbone.encoder.layer.1.mlp.fc2\n",
      "backbone.encoder.layer.1.layer_scale2\n",
      "backbone.encoder.layer.2\n",
      "backbone.encoder.layer.2.norm1\n",
      "backbone.encoder.layer.2.attention\n",
      "backbone.encoder.layer.2.attention.attention\n",
      "backbone.encoder.layer.2.attention.attention.query\n",
      "backbone.encoder.layer.2.attention.attention.key\n",
      "backbone.encoder.layer.2.attention.attention.value\n",
      "backbone.encoder.layer.2.attention.output\n",
      "backbone.encoder.layer.2.attention.output.dense\n",
      "backbone.encoder.layer.2.attention.output.dropout\n",
      "backbone.encoder.layer.2.layer_scale1\n",
      "backbone.encoder.layer.2.drop_path\n",
      "backbone.encoder.layer.2.norm2\n",
      "backbone.encoder.layer.2.mlp\n",
      "backbone.encoder.layer.2.mlp.fc1\n",
      "backbone.encoder.layer.2.mlp.activation\n",
      "backbone.encoder.layer.2.mlp.fc2\n",
      "backbone.encoder.layer.2.layer_scale2\n",
      "backbone.encoder.layer.3\n",
      "backbone.encoder.layer.3.norm1\n",
      "backbone.encoder.layer.3.attention\n",
      "backbone.encoder.layer.3.attention.attention\n",
      "backbone.encoder.layer.3.attention.attention.query\n",
      "backbone.encoder.layer.3.attention.attention.key\n",
      "backbone.encoder.layer.3.attention.attention.value\n",
      "backbone.encoder.layer.3.attention.output\n",
      "backbone.encoder.layer.3.attention.output.dense\n",
      "backbone.encoder.layer.3.attention.output.dropout\n",
      "backbone.encoder.layer.3.layer_scale1\n",
      "backbone.encoder.layer.3.drop_path\n",
      "backbone.encoder.layer.3.norm2\n",
      "backbone.encoder.layer.3.mlp\n",
      "backbone.encoder.layer.3.mlp.fc1\n",
      "backbone.encoder.layer.3.mlp.activation\n",
      "backbone.encoder.layer.3.mlp.fc2\n",
      "backbone.encoder.layer.3.layer_scale2\n",
      "backbone.encoder.layer.4\n",
      "backbone.encoder.layer.4.norm1\n",
      "backbone.encoder.layer.4.attention\n",
      "backbone.encoder.layer.4.attention.attention\n",
      "backbone.encoder.layer.4.attention.attention.query\n",
      "backbone.encoder.layer.4.attention.attention.key\n",
      "backbone.encoder.layer.4.attention.attention.value\n",
      "backbone.encoder.layer.4.attention.output\n",
      "backbone.encoder.layer.4.attention.output.dense\n",
      "backbone.encoder.layer.4.attention.output.dropout\n",
      "backbone.encoder.layer.4.layer_scale1\n",
      "backbone.encoder.layer.4.drop_path\n",
      "backbone.encoder.layer.4.norm2\n",
      "backbone.encoder.layer.4.mlp\n",
      "backbone.encoder.layer.4.mlp.fc1\n",
      "backbone.encoder.layer.4.mlp.activation\n",
      "backbone.encoder.layer.4.mlp.fc2\n",
      "backbone.encoder.layer.4.layer_scale2\n",
      "backbone.encoder.layer.5\n",
      "backbone.encoder.layer.5.norm1\n",
      "backbone.encoder.layer.5.attention\n",
      "backbone.encoder.layer.5.attention.attention\n",
      "backbone.encoder.layer.5.attention.attention.query\n",
      "backbone.encoder.layer.5.attention.attention.key\n",
      "backbone.encoder.layer.5.attention.attention.value\n",
      "backbone.encoder.layer.5.attention.output\n",
      "backbone.encoder.layer.5.attention.output.dense\n",
      "backbone.encoder.layer.5.attention.output.dropout\n",
      "backbone.encoder.layer.5.layer_scale1\n",
      "backbone.encoder.layer.5.drop_path\n",
      "backbone.encoder.layer.5.norm2\n",
      "backbone.encoder.layer.5.mlp\n",
      "backbone.encoder.layer.5.mlp.fc1\n",
      "backbone.encoder.layer.5.mlp.activation\n",
      "backbone.encoder.layer.5.mlp.fc2\n",
      "backbone.encoder.layer.5.layer_scale2\n",
      "backbone.encoder.layer.6\n",
      "backbone.encoder.layer.6.norm1\n",
      "backbone.encoder.layer.6.attention\n",
      "backbone.encoder.layer.6.attention.attention\n",
      "backbone.encoder.layer.6.attention.attention.query\n",
      "backbone.encoder.layer.6.attention.attention.key\n",
      "backbone.encoder.layer.6.attention.attention.value\n",
      "backbone.encoder.layer.6.attention.output\n",
      "backbone.encoder.layer.6.attention.output.dense\n",
      "backbone.encoder.layer.6.attention.output.dropout\n",
      "backbone.encoder.layer.6.layer_scale1\n",
      "backbone.encoder.layer.6.drop_path\n",
      "backbone.encoder.layer.6.norm2\n",
      "backbone.encoder.layer.6.mlp\n",
      "backbone.encoder.layer.6.mlp.fc1\n",
      "backbone.encoder.layer.6.mlp.activation\n",
      "backbone.encoder.layer.6.mlp.fc2\n",
      "backbone.encoder.layer.6.layer_scale2\n",
      "backbone.encoder.layer.7\n",
      "backbone.encoder.layer.7.norm1\n",
      "backbone.encoder.layer.7.attention\n",
      "backbone.encoder.layer.7.attention.attention\n",
      "backbone.encoder.layer.7.attention.attention.query\n",
      "backbone.encoder.layer.7.attention.attention.key\n",
      "backbone.encoder.layer.7.attention.attention.value\n",
      "backbone.encoder.layer.7.attention.output\n",
      "backbone.encoder.layer.7.attention.output.dense\n",
      "backbone.encoder.layer.7.attention.output.dropout\n",
      "backbone.encoder.layer.7.layer_scale1\n",
      "backbone.encoder.layer.7.drop_path\n",
      "backbone.encoder.layer.7.norm2\n",
      "backbone.encoder.layer.7.mlp\n",
      "backbone.encoder.layer.7.mlp.fc1\n",
      "backbone.encoder.layer.7.mlp.activation\n",
      "backbone.encoder.layer.7.mlp.fc2\n",
      "backbone.encoder.layer.7.layer_scale2\n",
      "backbone.encoder.layer.8\n",
      "backbone.encoder.layer.8.norm1\n",
      "backbone.encoder.layer.8.attention\n",
      "backbone.encoder.layer.8.attention.attention\n",
      "backbone.encoder.layer.8.attention.attention.query\n",
      "backbone.encoder.layer.8.attention.attention.key\n",
      "backbone.encoder.layer.8.attention.attention.value\n",
      "backbone.encoder.layer.8.attention.output\n",
      "backbone.encoder.layer.8.attention.output.dense\n",
      "backbone.encoder.layer.8.attention.output.dropout\n",
      "backbone.encoder.layer.8.layer_scale1\n",
      "backbone.encoder.layer.8.drop_path\n",
      "backbone.encoder.layer.8.norm2\n",
      "backbone.encoder.layer.8.mlp\n",
      "backbone.encoder.layer.8.mlp.fc1\n",
      "backbone.encoder.layer.8.mlp.activation\n",
      "backbone.encoder.layer.8.mlp.fc2\n",
      "backbone.encoder.layer.8.layer_scale2\n",
      "backbone.encoder.layer.9\n",
      "backbone.encoder.layer.9.norm1\n",
      "backbone.encoder.layer.9.attention\n",
      "backbone.encoder.layer.9.attention.attention\n",
      "backbone.encoder.layer.9.attention.attention.query\n",
      "backbone.encoder.layer.9.attention.attention.key\n",
      "backbone.encoder.layer.9.attention.attention.value\n",
      "backbone.encoder.layer.9.attention.output\n",
      "backbone.encoder.layer.9.attention.output.dense\n",
      "backbone.encoder.layer.9.attention.output.dropout\n",
      "backbone.encoder.layer.9.layer_scale1\n",
      "backbone.encoder.layer.9.drop_path\n",
      "backbone.encoder.layer.9.norm2\n",
      "backbone.encoder.layer.9.mlp\n",
      "backbone.encoder.layer.9.mlp.fc1\n",
      "backbone.encoder.layer.9.mlp.activation\n",
      "backbone.encoder.layer.9.mlp.fc2\n",
      "backbone.encoder.layer.9.layer_scale2\n",
      "backbone.encoder.layer.10\n",
      "backbone.encoder.layer.10.norm1\n",
      "backbone.encoder.layer.10.attention\n",
      "backbone.encoder.layer.10.attention.attention\n",
      "backbone.encoder.layer.10.attention.attention.query\n",
      "backbone.encoder.layer.10.attention.attention.key\n",
      "backbone.encoder.layer.10.attention.attention.value\n",
      "backbone.encoder.layer.10.attention.output\n",
      "backbone.encoder.layer.10.attention.output.dense\n",
      "backbone.encoder.layer.10.attention.output.dropout\n",
      "backbone.encoder.layer.10.layer_scale1\n",
      "backbone.encoder.layer.10.drop_path\n",
      "backbone.encoder.layer.10.norm2\n",
      "backbone.encoder.layer.10.mlp\n",
      "backbone.encoder.layer.10.mlp.fc1\n",
      "backbone.encoder.layer.10.mlp.activation\n",
      "backbone.encoder.layer.10.mlp.fc2\n",
      "backbone.encoder.layer.10.layer_scale2\n",
      "backbone.encoder.layer.11\n",
      "backbone.encoder.layer.11.norm1\n",
      "backbone.encoder.layer.11.attention\n",
      "backbone.encoder.layer.11.attention.attention\n",
      "backbone.encoder.layer.11.attention.attention.query\n",
      "backbone.encoder.layer.11.attention.attention.key\n",
      "backbone.encoder.layer.11.attention.attention.value\n",
      "backbone.encoder.layer.11.attention.output\n",
      "backbone.encoder.layer.11.attention.output.dense\n",
      "backbone.encoder.layer.11.attention.output.dropout\n",
      "backbone.encoder.layer.11.layer_scale1\n",
      "backbone.encoder.layer.11.drop_path\n",
      "backbone.encoder.layer.11.norm2\n",
      "backbone.encoder.layer.11.mlp\n",
      "backbone.encoder.layer.11.mlp.fc1\n",
      "backbone.encoder.layer.11.mlp.activation\n",
      "backbone.encoder.layer.11.mlp.fc2\n",
      "backbone.encoder.layer.11.layer_scale2\n",
      "backbone.layernorm\n",
      "neck\n",
      "neck.reassemble_stage\n",
      "neck.reassemble_stage.layers\n",
      "neck.reassemble_stage.layers.0\n",
      "neck.reassemble_stage.layers.0.projection\n",
      "neck.reassemble_stage.layers.0.resize\n",
      "neck.reassemble_stage.layers.1\n",
      "neck.reassemble_stage.layers.1.projection\n",
      "neck.reassemble_stage.layers.1.resize\n",
      "neck.reassemble_stage.layers.2\n",
      "neck.reassemble_stage.layers.2.projection\n",
      "neck.reassemble_stage.layers.2.resize\n",
      "neck.reassemble_stage.layers.3\n",
      "neck.reassemble_stage.layers.3.projection\n",
      "neck.reassemble_stage.layers.3.resize\n",
      "neck.convs\n",
      "neck.convs.0\n",
      "neck.convs.1\n",
      "neck.convs.2\n",
      "neck.convs.3\n",
      "neck.fusion_stage\n",
      "neck.fusion_stage.layers\n",
      "neck.fusion_stage.layers.0\n",
      "neck.fusion_stage.layers.0.projection\n",
      "neck.fusion_stage.layers.0.residual_layer1\n",
      "neck.fusion_stage.layers.0.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.0.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.0.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.0.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.0.residual_layer2\n",
      "neck.fusion_stage.layers.0.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.0.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.0.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.0.residual_layer2.convolution2\n",
      "neck.fusion_stage.layers.1\n",
      "neck.fusion_stage.layers.1.projection\n",
      "neck.fusion_stage.layers.1.residual_layer1\n",
      "neck.fusion_stage.layers.1.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.1.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.1.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.1.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.1.residual_layer2\n",
      "neck.fusion_stage.layers.1.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.1.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.1.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.1.residual_layer2.convolution2\n",
      "neck.fusion_stage.layers.2\n",
      "neck.fusion_stage.layers.2.projection\n",
      "neck.fusion_stage.layers.2.residual_layer1\n",
      "neck.fusion_stage.layers.2.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.2.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.2.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.2.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.2.residual_layer2\n",
      "neck.fusion_stage.layers.2.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.2.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.2.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.2.residual_layer2.convolution2\n",
      "neck.fusion_stage.layers.3\n",
      "neck.fusion_stage.layers.3.projection\n",
      "neck.fusion_stage.layers.3.residual_layer1\n",
      "neck.fusion_stage.layers.3.residual_layer1.activation1\n",
      "neck.fusion_stage.layers.3.residual_layer1.convolution1\n",
      "neck.fusion_stage.layers.3.residual_layer1.activation2\n",
      "neck.fusion_stage.layers.3.residual_layer1.convolution2\n",
      "neck.fusion_stage.layers.3.residual_layer2\n",
      "neck.fusion_stage.layers.3.residual_layer2.activation1\n",
      "neck.fusion_stage.layers.3.residual_layer2.convolution1\n",
      "neck.fusion_stage.layers.3.residual_layer2.activation2\n",
      "neck.fusion_stage.layers.3.residual_layer2.convolution2\n",
      "head\n",
      "head.conv1\n",
      "head.conv2\n",
      "head.activation1\n",
      "head.conv3\n",
      "head.activation2\n"
     ]
    }
   ],
   "source": [
    "# Afficher les noms de toutes les couches (modules) du modèle\n",
    "print(\"Noms de toutes les couches du modèle :\")\n",
    "for name, module in model.named_modules():\n",
    "    if name:  # Éviter la racine vide\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073cf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration LoRA Correcte pour la Vision\n",
    "# On cible tous les modules linéaires du Transformer pour un meilleur apprentissage\n",
    "# On retire 'task_type' pour éviter l'erreur \"input_ids\"\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\", \"fc1\", \"fc2\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77586d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,327,104 || all params: 26,112,193 || trainable%: 5.0823\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17db4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Trainer Personnalisé pour gérer la Loss et les NaNs\n",
    "class DepthTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "        \n",
    "        # L'output du modèle peut être légèrement différent de la taille d'entrée (padding)\n",
    "        # On s'assure que la prédiction matche les labels\n",
    "        if predicted_depth.shape[-2:] != labels.shape[-2:]:\n",
    "            predicted_depth = F.interpolate(\n",
    "                predicted_depth.unsqueeze(1), \n",
    "                size=labels.shape[-2:], \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            ).squeeze(1)\n",
    "\n",
    "        # Masquage des valeurs invalides (NaNs ou inf)\n",
    "        # On suppose que la profondeur valide est > 0 et n'est pas NaN\n",
    "        valid_mask = ~torch.isnan(labels) & ~torch.isinf(labels) & (labels > 0)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predicted_depth.device, requires_grad=True)\n",
    "\n",
    "        # Calcul de la Loss (L1 Loss est souvent mieux pour la profondeur que MSE)\n",
    "        loss = F.l1_loss(predicted_depth[valid_mask], labels[valid_mask])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d10e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Arguments d'entraînement\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output_depth_lora\",\n",
    "    remove_unused_columns=False, # Important pour garder 'labels'\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4, # Un peu plus bas pour LoRA\n",
    "    per_device_train_batch_size=4, # Ajuste selon ta VRAM (128 est énorme pour des images)\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4, # Simule un batch plus grand\n",
    "    fp16=True,\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"], # Indique au Trainer de ne pas supprimer cette colonne\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc639693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de collation simple\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8499ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\Git\\LoRA-Depth-Anything\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1357: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 09:40, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>329.557007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>325.189728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>318.726959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>323.916500</td>\n",
       "      <td>315.867462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>323.916500</td>\n",
       "      <td>313.379425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>323.916500</td>\n",
       "      <td>311.803223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>312.404000</td>\n",
       "      <td>310.674713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>312.404000</td>\n",
       "      <td>309.612213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>312.404000</td>\n",
       "      <td>308.709229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>312.266300</td>\n",
       "      <td>307.853241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>312.266300</td>\n",
       "      <td>307.104462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>312.266300</td>\n",
       "      <td>306.426575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>312.266300</td>\n",
       "      <td>305.819702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>306.894700</td>\n",
       "      <td>305.285614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>306.894700</td>\n",
       "      <td>304.795654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>306.894700</td>\n",
       "      <td>304.382996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>307.922400</td>\n",
       "      <td>304.016846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>307.922400</td>\n",
       "      <td>303.715424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>307.922400</td>\n",
       "      <td>303.453766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>305.641900</td>\n",
       "      <td>303.247345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>305.641900</td>\n",
       "      <td>303.075592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>305.641900</td>\n",
       "      <td>302.964478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>305.641900</td>\n",
       "      <td>302.913422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>301.073600</td>\n",
       "      <td>302.796661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>301.073600</td>\n",
       "      <td>302.731964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>301.073600</td>\n",
       "      <td>302.676392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>305.502500</td>\n",
       "      <td>302.630157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>305.502500</td>\n",
       "      <td>302.577881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>305.502500</td>\n",
       "      <td>302.532928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>303.370500</td>\n",
       "      <td>302.501373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>303.370500</td>\n",
       "      <td>302.476715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>303.370500</td>\n",
       "      <td>302.440247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>303.370500</td>\n",
       "      <td>302.411743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>306.163200</td>\n",
       "      <td>302.392975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>306.163200</td>\n",
       "      <td>302.368896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>306.163200</td>\n",
       "      <td>302.349152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>299.843600</td>\n",
       "      <td>302.331543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>299.843600</td>\n",
       "      <td>302.313904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>299.843600</td>\n",
       "      <td>302.304535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>301.321600</td>\n",
       "      <td>302.288666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>301.321600</td>\n",
       "      <td>302.276764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>301.321600</td>\n",
       "      <td>302.266083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>301.321600</td>\n",
       "      <td>302.255096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>301.721900</td>\n",
       "      <td>302.247650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>301.721900</td>\n",
       "      <td>302.241791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>301.721900</td>\n",
       "      <td>302.236847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>305.243700</td>\n",
       "      <td>302.231720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>305.243700</td>\n",
       "      <td>302.228424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>305.243700</td>\n",
       "      <td>302.225281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>301.642100</td>\n",
       "      <td>302.223907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=306.3285725911458, metrics={'train_runtime': 585.3879, 'train_samples_per_second': 3.929, 'train_steps_per_second': 0.256, 'total_flos': 4.70384209111824e+17, 'train_loss': 306.3285725911458, 'epoch': 50.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Lancement\n",
    "trainer = DepthTrainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6000f0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 8️⃣ Test sur un exemple du dataset test\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      4\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m sample = \u001b[43mtest_dataset\u001b[49m[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# choisir le premier exemple\u001b[39;00m\n\u001b[32m      6\u001b[39m image = sample[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m      7\u001b[39m depth_gt = sample[\u001b[33m\"\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 8️⃣ Tests graphiques sur le dataset de test\n",
    "# =========================\n",
    "import random\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Sélectionner quelques exemples aléatoires du dataset de test\n",
    "num_samples = 3\n",
    "indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    sample = test_dataset[idx]\n",
    "    pixel_values = sample['pixel_values'].unsqueeze(0).to(device)\n",
    "    labels = sample['labels']\n",
    "    image = sample['image']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        pred_depth = outputs.predicted_depth.squeeze(0).cpu()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Image RGB {i+1}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Depth map vraie\")\n",
    "    plt.imshow(labels.numpy(), cmap=\"plasma\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Depth map prédite\")\n",
    "    plt.imshow(pred_depth.numpy(), cmap=\"plasma\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872ebe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e2e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c1c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
